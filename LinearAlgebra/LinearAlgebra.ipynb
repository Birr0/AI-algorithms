{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra for Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "1) Deep Learning - Ch2 Linear Algebra (Goodfellow et. al)\n",
    "\n",
    "2) Deep Learning - Appendix A (Bishop and Bishop)\n",
    "\n",
    "3) Essence of linear algebra - 3Blue1Brown - https://www.youtube.com/watch?v=kYB8IZa5AuE&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Alegrba concerns several types of mathematical objects:\n",
    "\n",
    "1) Scalars: Single numbers\n",
    "\n",
    "2) Vectors: An ordered array of numbers (matrices with a single column)\n",
    "\n",
    "3) Matrices: A 2D array of numbers\n",
    "\n",
    "4) Tensors: An n-dimensional array of numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose\n",
    "\n",
    "An important operation in linear algebra is the transpose. The transpose of a matrix is obtained by swapping its rows and columns. It can also be though of as making a mirror image of the matrix across the main or leading diagonal. If $A$ is a matrix, the transpose of $A$ is denoted as $A^T$. It is defined as:\n",
    "\n",
    "$(A^T)_{ij} = A_{ji}$\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\begin{bmatrix}\n",
    "A_1 & A_2 & A_3\\\\\n",
    "A_4 & A_5 & A_6\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow\n",
    "A^T = \\begin{bmatrix}\n",
    "A_1 & A_4\\\\\n",
    "A_2 & A_5\\\\\n",
    "A_3 & A_6\n",
    "\\end{bmatrix}\n",
    "\\end{equation} \n",
    "*In the case of a vector, the transpose of a vector is a row vector. An for a scalar, the transpose is the scalar itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Addition and Scalar Multiplication\n",
    "Matrices can be added to each other, provided that the have the same dimensions. In this case the sum is obtained by adding the corresponding elements of the two matrices.\n",
    "\n",
    "Matrices can be multiplied by scalars, obtained by multiplying each element of the matrix by the scalar. Or they can add scalars to matrices, obtained by adding the scalar to each element of the matrix.\n",
    "\n",
    "In the context of deep learning, vectors or matrices can be added to matrices even if they have different dimensions through a process called broadcasting. This requires the vector or matrix of smaller dimension to be extended to match the dimensions of the larger matrix, given that the dimensions of the smaller one are compatible with the larger one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix multiplication of two matrices $A$ and $B$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "C = AB\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "C_{ij} = \\sum_k A_{i,k}B_{k,j}\n",
    "\\end{equation}\n",
    "\n",
    "In order for this matrix multiplication to be broadcasted, the number of columns in $A$ must be equal to the number of rows in $B$ i.e $m \\times n$ and $n \\times q$ and so the shape of $C$ will be $m \\times q$.\n",
    "\n",
    "Note: there also exists the element-wise or Hadamard product multiplication of two matrices, denotes by $A \\odot B$ which is obtained by multiplying the corresponding elements of the two matrices. For example:\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\begin{bmatrix}\n",
    "A_1 & A_2\\\\\n",
    "A_3 & A_4\n",
    "\\end{bmatrix}\n",
    "B = \\begin{bmatrix}\n",
    "B_1 & B_2\\\\\n",
    "B_3 & B_4\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow\n",
    "A \\odot B = \\begin{bmatrix}\n",
    "A_1B_1 & A_2B_2\\\\\n",
    "A_3B_3 & A_4B_4\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Matrix multiplication are linear transformations of space - a movement of the basis. \n",
    "\n",
    "(Linear if all lines must remain lines and origin must remain in place. Or more formally, a transformation L is linear if it satisifies the following properties: \n",
    "\n",
    "1) $L(\\textbf{v} + \\textbf{w}) = L(\\textbf{v}) + L(\\textbf{w})$ - \"Additivity\"\n",
    "\n",
    "2) $L(c \\textbf{v}) = c \\ L(\\textbf{v})$ - \"Scaling\"\n",
    "\n",
    "i.e. Keeps grid lines parallel and evenly spaced.) \n",
    "\n",
    "The columns of $A$ above denote where the basis vectors $\\hat{i}$ and $\\hat{j}$ are transformed to.\n",
    "\n",
    "For example, the $90^\\circ$ rotation counterclockwise of the basis is given by :\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\begin{bmatrix}\n",
    "0 & -1\\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{equation} \n",
    "\n",
    "Multiple transformations applied in one single 'action' is known as a \"composition\". It's a product of individual matrices (read from right ,the first transformation, to left the final transformation).\n",
    "\n",
    "In general, for a matrix transformations $M_1$ and $M_2$:\n",
    "(Notice: $M_1 M_2 \\neq M_2 M_1$)\n",
    "\n",
    "\\begin{equation}\n",
    "M_2 \\times M_1 = \\begin{bmatrix}\n",
    "a & b\\\\\n",
    "c & d\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "e & f\\\\\n",
    "g & h\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "ae + bg & af + bh\\\\\n",
    "ce + dg & cf +dh\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "The dot product between two vectors $a$ and $b$ which are of the same dimensionality is the matrix product $a^Tb$:\n",
    "\n",
    "\\begin{equation}\n",
    "a \\cdot b = \\sum_{i =1}^{n} a_ib_i\n",
    "\\end{equation}\n",
    "For example:\n",
    "\n",
    "\\begin{equation}\n",
    "a \\cdot b = \\begin{bmatrix}\n",
    "a_1\\\\\n",
    "a_2\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "b_1\\\\\n",
    "b_2 \n",
    "\\end{bmatrix} = a_1b_1 + a_2b_2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of matrix multiplication\n",
    "\n",
    "Distrubutive: \n",
    "\n",
    "$A(B + C) = AB + BC$\n",
    "\n",
    "Associative:\n",
    "\n",
    "$ A(BC) = (AB)C$\n",
    "\n",
    "Not commutative:\n",
    "\n",
    "$ AB \\neq BA$\n",
    "*Except for vectors\n",
    "\n",
    "The transpose of a matrix product has the form:\n",
    "$(AB)^T = B^TA^T$\n",
    "\n",
    "(which can be verified by writing out the indicies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of linear equations in linear algebra\n",
    "\n",
    "A system of linear equations can be written compactly using linear algebra notation:\n",
    "\n",
    "\\begin{equation}\n",
    "Ax = b\n",
    "\\end{equation}\n",
    "\n",
    "where $A_{i, :} x = \\sum_{k = 1}^{n} A_{i, k}x_k = b_i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity and Inverse matrices\n",
    "\n",
    "Linear alegra offers a convient way of solving such linear equations using matrix inversion that enables analytical solutions to be found for may values of A. \n",
    "\n",
    "Essential to understanding matrix inversion is the idenity matrix, $I_n$ where $I_n \\in \\mathbb{R}^{nxn}$ and by definition:\n",
    "\n",
    "\\begin{equation}\n",
    "\\forall x \\in \\mathbb{R}^{n}, I_n x = x\n",
    "\\end{equation}\n",
    "\n",
    "The structure of the identity matrix is that all of the entries along the main diagonal are 1. i.e.\n",
    "\n",
    "$I_2 = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The matrix inverse, denoted as $A^{-1}$ is defined such that:\n",
    "\n",
    "\\begin{equation}\n",
    "A^{-1}A = I_n\n",
    "\\end{equation}\n",
    "\n",
    "This allows the linear equations to be solved:\n",
    "\n",
    "$Ax = b$\n",
    "\n",
    "$A^{-1}Ax = A^{-1}b$\n",
    "\n",
    "$I_nx = A^{-1}b$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $A^{-1} to exist, $Ax = b$ must have one solution for every $b$. It is possible for for the system of equations to have no solutions or infinitely many solutions for some values of $b$. It is not possible to have more than one but less than infinite solutions for a particular $b$ if both $x$ and $y$ are solutions, then\n",
    "\n",
    "$ z = \\alpha x + (1 - \\alpha)y $ is also a solution for any real $a$.\n",
    "\n",
    "To find how many solutions the equation has, think of the columns of $A$ as specifying different directions that can be travelled from the origin, then determine how many ways there are of reaching $b$. This means each element of $x$ specifies how far to travel in each of these directions to reach $b$, i.e. $x_i$ specifies how far to travel in the direction of column $i$:\n",
    "\n",
    "$Ax = \\sum_{i} x_i A_{:, i}$\n",
    "\n",
    "In general, this operation is a called a linear combination which is defined formally as multipying each vector in a set $\\{ v^{(1)},...,v^{(n)} \\} $ by a scalar coefficient and adding the results:\n",
    "\n",
    "$ \\sum_{i} c_{i} v^{(i)}$\n",
    "\n",
    "The **span** of a set of vectors is the set of all points obtainable by a linear combination of thr original vectors. Therefore determining whether there is a solution to the linear equation amounts to testing whether $b$ is in the span of the columns of $A$ - this particular span is known as the column space or the range of $A$. So for the system to have a solution for $b \\in \\mathbb{R}^m$ then the column space of $A$ must be $\\mathbb{R}^m$. So if any point in $\\mathbb{R}^m$ is excluded from the column space of $A$, that point is a potential value of $b$ that has no solution. \n",
    "\n",
    "The requirement that the column space of $A$ be all of $\\mathbb{R}^m$ means that $A$ must have at least $m$ columns (and that for numbers of columns greater than $m$ they are not redundant i.e repition of columns). For example, consider a 3Ã—2 matrix. The target b is 3-D, but x is only 2-D, so modifying the value of x at best enables us to trace out a 2-D plane within $\\mathbb{R}^3$. The equation has a solution if and only if b lies on that plane.\n",
    "\n",
    "The point of redunancy above is formally known as linear dependence. A set of vectors is linearlly independent if no vector in the set is a linear combination of the other vectors. If a new vector is added to a set that is a linear combination of the other vectors, that vector does not add any points to the set's span. This means that that for the column space of the matrix to encompass all of $\\mathbb{R}^m$ the matrix must contain at least one set of $m$ linearly indepdent columns in order for the linear equation to have a solution for every value of $b$. \n",
    "\n",
    "A set of vectors $\\{ x_1, ..., x_n \\}$ is said to be linearly independent if the relation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_n \\alpha_n x_n = 0\n",
    "\\end{equation}\n",
    "\n",
    "holds only if all $\\alpha_n = 0$. This implies that none of the vectors can be expressed as a linear combination of the remainder. The rank of a matrix is the maximum number of linearly independent rows (or equivalently, the maximum number of linearly independent columns).\n",
    "\n",
    "Additionally, for the linear equation to have an inverse, it must be ensured that the linear equation has at most one solution for each value of $b$. This means that $A$ must have $m$ columns and no more or less; otherwise, there is more than one way of parameterizing each solution. \n",
    "\n",
    "All togther, this means that the matrix must be square. \n",
    "\n",
    "A square matrix with linearly dependent columns is known as **singular**\n",
    "\n",
    "If $A$ is not square or is square but singular, solving the equations is possible but not through matrix inversion. \n",
    "\n",
    "Note: it is also possible to define an inverse matrix that is multipled on the right:\n",
    "\n",
    "\\begin{equation}\n",
    " AA^{-1} = I\n",
    "\\end{equation}\n",
    "\n",
    "For square matrices, the left and right inverse are equal.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "The size of a vector is calculated using the **norm** (intuitively this is the distance from the origin to $x$). Formally, the $L^{p}$ norm is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "||x||_p = (\\sum_i |x_i|^{p})^{\\frac{1}{p}}\n",
    "\\end{equation}\n",
    "\n",
    "for $p \\in \\mathbb{R}$, $p \\geq 1$\n",
    "\n",
    "Rigorously, the norm must satisy:\n",
    "\n",
    "1) $f(x) = 0$ $\\Rightarrow$ $x = 0$\n",
    "2) $f(x + y) \\leq f(x) + f(y)$ (triangle inequality)\n",
    "3) $\\forall \\alpha \\in \\mathbb{R}$, $f(\\alpha x) = |\\alpha|f(x)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $L^2$ norm is known as the Eucliean norm (i.e. the Euclidean distance from the origin to the point identified by $x$). Given its frequent use in machine learning, it is often dentoed as $||x||$. It is also common to measure the size of a vector using the squared $L^2$ norm which is $L^2 = x^Tx$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many ML applications, and in some the following distinction is essential, it is important to discriminate between elements that are zero and those that are small but nonzero. In these cases the $L^1$ norm is used. Every time an element of $x$ moves from 0 by $\\epsilon$, the $L^1$ norm increases by $\\epsilon$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max norm, $L^\\infty$, is absolute value of the element with the largest magnitude in the vector:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "||x||_\\infty = max_i |x_i|\n",
    "\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Frobenius norm** is the size of a matrix (analogous to the L^2 norm of a vector):\n",
    "\n",
    "\\begin{equation}\n",
    "||A||_F = \\sqrt{\\sum_{i,j} A^2_{i, j}}\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product of two vectors can be rewritten in terms of norms:\n",
    "\n",
    "\\begin{equation}\n",
    "x^Ty = ||x||_2||y||_2 \\cos (\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta$ is the angle between $x$ and $y$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special kinds of matrices and vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagonal** matrices consist of nonzero entries only along the main diagonal. Formally, a matrix $D$ is diagonal if and only if $D_{i, j} = 0$ for all $i = j$. ($diag(v)$ to denote a square diagonal matrix whose diagonal entries are given by the entries of the vector v). Multiplication by a diagonal matrix is computationally efficient. (i.e. $diag(v)x = v \\odot x$). Inversion of diagonal sqaure matrices are also efficient (if every diagonal entry is nonzero) $diag(v)^{-1} = diag([\\frac{1}{v_1},...,\\frac{1}{v_n}]^T)$. \n",
    "\n",
    "In many cases, a general ML algorithm may be derived in terms of arbitary matrices but a less expensive (and less descriptive) algorithm can be obtained by restricting some matrices to be diagonal. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symmetric matrix**: $A = A^T$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit vector**: $||x||_2 = 1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orthogonal vectors**: A vector $x$ and $y$ are orthogonal to each other if $x^Ty = 0$. (If both vectors have nonzero norm, this means they are at $90^\\circ$ to each other). \n",
    "\n",
    "In $\\mathbb{R}^{n}$, at most n vectors may be mutually orthogonal with nonzero norm.\n",
    "\n",
    "**Orthonormal**: Vectors that are orthogonal and also have unit norm.\n",
    "\n",
    "**Orthogonal matrix**: A square matrix whoses rows are mutually **orthonormal** and whoses columns are mutually **orthonormal**:\n",
    "\n",
    "\\begin{equation}\n",
    "A^TA = AA^T = I\n",
    "\\end{equation}\n",
    "\n",
    "which implies that $A^{-1} = A^T$.\n",
    "\n",
    "Orthogonal matrices are of interest as their inverse is very cheap to compute. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompistion of matrices can show information about their functional properties that are not obvious from the representation of the matrix as an array of elements. In a similar way that decomposing numbers into their primes can help in their analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigendecomposition: decompose a matrix into a set of eigenvalues and eigenvectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An eigenvector of a square matrix ($M \\times M$) $A$ is a nonzero vector $v$ such that multiplication by $A$ alters only the scale of $v$:\n",
    "\n",
    "\\begin{equation}\n",
    "Av = \\lambda v\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is a scalar known as an eigenvalue corresponding to the eigenvector.\n",
    "\n",
    "This can be viewed as a set of $M$ simultaneous homogenous linear equations, and the condition for solution known as the **characteristic equation** is \n",
    "\n",
    "\\begin{equation}\n",
    "|A - \\lambda I | = 0\n",
    "\\end{equation}\n",
    "\n",
    "As this is a polynomial of order $M$ in $\\lambda$ it must have $M$ solutions (though they do not need to be all distinct). The rank of $A$ is equal to the number of non-zero eigenvalues.\n",
    "\n",
    "If $v$ is an eigenvector if $A$ then so is any rescaled vector $sv$ for $s \\in \\mathbb{R}$, $s \\neq 0$. $sv$ has the same eigenvalue and for this reason, only unit eigenvectors are usually considered. \n",
    "\n",
    "If matrix $A$ has $n$ linearly independent eigenvectors $\\{ v^{(1)},...,v^{(n)} \\}$ with corresponding eigenvalues $\\{ \\lambda^{(1)},...,\\lambda^{(n)} \\}$, then all of the eigenvectors can be concatenated to form a matrix, $V$ with one eigenvector per column: $V = [v^{(1)},...,v^{(n)}]$ and the eigenvalues can be concatenated to form a vector $\\lambda = [\\lambda^{(1)},...,\\lambda^{(n)}]$. The **eigendecomposition** of $A$ is then given by:\n",
    "\n",
    "\\begin{equation}\n",
    "A = V diag(\\lambda) V^{-1}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases, the decomposition exists but may involve complex rather than real numbers. \n",
    "\n",
    "However, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:\n",
    "\n",
    "\\begin{equation}\n",
    "A = Q \\Lambda Q^T\n",
    "\\end{equation}\n",
    "\n",
    "where $Q$ is an orthogonal matrix composed of eigenvectors of $A$, and $\\Lambda$ is a diagonal matrix. The eigenvalue $\\Lambda_{i, i}$ is associated with the eigenvector in column $i$ of $Q$, denoted as $Q_{:, i}$. As $Q$ is an orthogonal matrix, $A$ can be thought of as scaling space by $\\lambda_i$ in direction $v^{(i)}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While any real symmetric matrix $A$ is guaranteed to have an eigendecomposition, the eigendecomposition may not be unique. If any two or more eigenvectors share the same eigenvalue, then any set of orthogonal vectors lying in their span are also eigenvectors with that eigenvalue, and a $Q$ could be choosen using those eigenvectors instead. By convention, the entries of $\\Lambda$ are sorted in descending order. Under this convention, the eigendecomposition is unique only if all the eigenvalues are unique."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigendecomposition reveals many useful facts about the matrix:\n",
    "\n",
    "1) The matrix is singular if and only if any of the eigenvalues are zero.\n",
    "2) The eigendecomposition of a real symmetric matrix can also be used to optimize quadratic expressions of the form $f(x) = x^TAx$ subject to $||x||_2 = 1$.\n",
    "3) Whenever $x$ is equal to an eigenvector of $A$, $f$ takes on the value of the corresponding eigenvalue. \n",
    "4) The maximum value of $f$ within the constraint region is the maximum eigenvalue and its minimum value within the constraint region is the minimum eigenvalue. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positive semidefinite matrix**: A matrix whose eigenvalues are all positive or zero\n",
    "\n",
    "Guarantee that $\\forall x, x^TAx \\geq 0$\n",
    "\n",
    "\n",
    "**Positive definite matrix**: A matrix whose eigenvalues are all positive\n",
    "\n",
    "Additionally guarantee that, $x^TAx = 0 \\Rightarrow x = 0$\n",
    "\n",
    "**Negative definite**: A matrix whose eigenvalues are all negative\n",
    "\n",
    "**Negative semidefinite**: A matrix whose eigenvalues are all negative or zero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition (SVD)\n",
    "\n",
    "SVD provides another way to factorize a matrix, into **singular vectors** and **singular values**. It allows some of the same information to be revealed as in eigendecomposition but it is more generally applicable; every real matrix has a singular value decomposition but the same is not true of eigenvalue decomposition (i.e. if a matrix is not square, the eigendecomposition is not defined and SVD must be used instead)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In eigendecomposition, a matrix $A$ is analyzed to discover a matrix $V$ of eigenvectors and a vector of eigenvalues $\\lambda$ that can be used to write $A$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "A = V diag(\\lambda)V^{-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is similar, except $A$ will be written as the product of three matrices:\n",
    "\n",
    "\\begin{equation}\n",
    "A = UDV^T\n",
    "\\end{equation}\n",
    "\n",
    "If $A$ is an $m \\times n$ matrix, then $U$ is defined to be an $m \\times m$ matrix, $D$ is defined to be an $m \\times n$ matrix, and $V$ to be an $n \\times n$ matrix.\n",
    "\n",
    "The elements along the diagonal of $D$ are known as the singular values of the matrix $A$. The columns of $U$ are known as the **left-singular vectors**. The columns of $V$ are known as the **right-singular vectors**.\n",
    "\n",
    "The SVD of $A$ can be interpreted as the eigendecomposition of functions of $A$. The left-singular vectors of $A$ are the eigenvectors of $AA^T$. The right-singular vectors of $A$ are the eigenvectors of $A^TA$. The nonzero singular values of $A$ are the square roots of the eigenvalues of $A^TA$ and $AA^T$.\n",
    "\n",
    "One of the most useful features of SVD is that it can be used to partially generalize matrix inversion to nonsquare matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moore-Penrose pseudoinverse\n",
    "\n",
    "Matrix inversion is not defined for matrices that are not square. However, suppose there exists a left-inverse matrix $B$ of a matrix $A$ so that the following linear equation can be solved:\n",
    "\n",
    "\\begin{equation}\n",
    "Ax = y\n",
    "\\end{equation}\n",
    "\n",
    "with \n",
    "\n",
    "\\begin{equation}\n",
    "x = By\n",
    "\\end{equation}\n",
    "\n",
    "Depending on the structure of the problem, it may not be possible to design a unique mapping from $A$ to $B$. \n",
    "\n",
    "If $A$ is taller than it is wide, then it is possible for this equation to have no solution. If $A$ is wider than it is tall, then there could be multiple solutions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Moore-Penrose pseudoinverse enables headway to be made in these cases. The pseudoinverse of $A$ is defined as a matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "A^{+} = lim_{a \\rightarrow 0}(A^TA + \\alpha I)^{-1}A^T\n",
    "\\end{equation}\n",
    "\n",
    "Practical algorithms for computing the pseudoinverse are based on the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "A^{+} = VD^{+}U^{T}\n",
    "\\end{equation}\n",
    "\n",
    "where $U$, $D$ and $V$ are the singular value decomposition of $A$, and the pseudoinverse $D^{+}$ of a diagonal matrix $D$ is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix.\n",
    "\n",
    "When $A$ has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides the solution $x = A^{+}y$ with minimal Euclidean norm $||x||_2$ among all possible solutions.\n",
    "\n",
    "When $A$ has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives the $x$ for which $Ax$ is as close as possible to $y$ in terms of Euclidean norm $||Ax - y||_2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace operator\n",
    "\n",
    "Trace operator: the sum of all the diagonal entries of a matrix\n",
    "\n",
    "\\begin{equation}\n",
    "Tr(A) = \\sum_i A_{i,i}\n",
    "\\end{equation}\n",
    "\n",
    "The trace operator is useful for a variety of reasons. Some operations that are difficult to specify without resorting to summation notation can be specified using matrix products and the trace operator. \n",
    "\n",
    "For example, the trace operator provides an alternative way of writing the Frobenius norm of a matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "||A||_F = \\sqrt{Tr(AA^T)}\n",
    "\\end{equation}\n",
    "\n",
    "The trace operator is invariant to the transpose operator which is very useful for manipulation of expressions using indentities:\n",
    "\n",
    "$Tr(A) = Tr(A^T)$\n",
    "\n",
    "By writing out the indices, it is possible to see that:\n",
    "\n",
    "$Tr(AB) = Tr(BA)$\n",
    "\n",
    "The trace of a square matrix composed of many factors is also invariant to moving the last factor into the first position which can be seen from applying the above formula multiple times to the product of the three matrices:\n",
    "\n",
    "$Tr(ABC) = Tr(CAB) = Tr(BCA)$\n",
    "\n",
    "or more generally this cyclic property of the trace operator is known as,\n",
    "\n",
    "$Tr(\\prod_{i=1}^{n}F^{(i)}) = Tr(F^{(n)}\\prod_{i=1}^{n - 1}F^{(i)})$\n",
    "\n",
    "This invariance to cyclic permutation holds even if the resulting product has a different shape.\n",
    "\n",
    "*Note: A scalar is its own trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant\n",
    "\n",
    "Determinant: a function that maps matrices to real scalars for sqaure matrices. \n",
    "\n",
    "Intuitively when thinking about matrices as linear transformations, the magnitude of the determinant describes how the area of the basis changes under the transformation. (Determinant: The factor by which a linear transformation changes any area in a transformation.) Note: a negative sign in a determinant means that the transformation inverts the orientation of space.\n",
    "\n",
    "The determinant is equal to the product of all the eigenvalues of the matrix. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts space (i.e. if the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all of its volume. If the determinant is 1, then the transformation preserves volume).\n",
    "\n",
    "The determinant, $|A|$ of an $N \\times N$ matrix A is defined by\n",
    "\n",
    "\\begin{equation}\n",
    "|A| = \\sum (\\pm 1) A_{1,i1}A_{2,i2}...A_{N,iN}\n",
    "\\end{equation}\n",
    "\n",
    "in which the sum is taken over all products consiting of precisely one element from each row and one element from each column, with a coefficient $+1$ or $-1$ indicating whether the permutation is even or odd.\n",
    "\n",
    "For example, the determinant of a $2 \\times 2$ diagonal matrix is given by the product of the elements on the leading diagonal:\n",
    "\n",
    "\\begin{equation}\n",
    "|A| = \\begin{vmatrix}\n",
    "a_{11} & a_{12} \\\\ \n",
    "a_{21} & a_{22}\n",
    "\\end{vmatrix} = a_{11}a_{22} - a_{12}a_{21}\n",
    "\\end{equation}\n",
    "\n",
    "The determinant of a product of two matrices is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "|AB| = |A||B|\n",
    "\\end{equation}\n",
    "\n",
    "The determinant of an inverse matrix is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "|A^{-1}| = \\frac{1}{|A|}\n",
    "\\end{equation}\n",
    "\n",
    "which can be shown by taking the determinant of $AA^{-1} = A^{-1}A = I$ and applying the multiplication rule.\n",
    "\n",
    "If $A$ and $B$ are matrices of size $N \\times M$, then:\n",
    "\n",
    "\\begin{equation}\n",
    "|I_N + AB^T| = |I_M + A^TB|\n",
    "\\end{equation}\n",
    "\n",
    "A useful special case is\n",
    "\n",
    "\\begin{equation}\n",
    "|I_N + ab^T| = 1+ a^Tb\n",
    "\\end{equation}\n",
    "\n",
    "where $a$ and $b$ are $N-$dimensional column vectors.\n",
    "\n",
    "A determinant of $|A| = 0$ 'squashes' all of space onto a line or a single point. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)\n",
    "\n",
    "A simple ML algorithm that can be derived exclusively with linear algebra.\n",
    "\n",
    "Suppose there is a collection of m points $\\{ x^{(1)},..., x^{(m)} \\}$ in $\\mathbb{R}$. PCA is a lossy compression method for those points (i.e. a way of storing those points that requires less memory but may lose some precision.)\n",
    "\n",
    "One way to encode these points is to represent a lower-dimensional version of them. For each point $x^{(i)} \\in \\mathbb{R}^{n}$, a corresponding code vector $c^{(i)} \\in \\mathbb{R}^{l}$. If $l < n$ then storing the code points will take less memory than storing the original data. In order to do this, an encoding function is required such that:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = c\n",
    "\\end{equation}\n",
    "\n",
    "and also, a decoding function (g) is required that reproduces the input given the code:\n",
    "\n",
    "\\begin{equation}\n",
    "x \\approx g(f(x))\n",
    "\\end{equation}\n",
    "\n",
    "PCA is defined by the choice of the decoding function. To make the decoder very simple, matrix multiplication is used to map the code back into $\\mathbb{R}^{n}$. Let $g(c) = Dc$, where $D \\in \\mathbb{R}^{n \\times l}$ is the matrix defining the decoding.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the optimal code for the decoder could be a difficult problem. In order to keep the encoding problem easy, PCA constrains the columns of $D$ to be orthogonal to each other. ($D$ is not technivally orthogonal unless $l = n$).\n",
    "\n",
    "Many solutions are possible to the above prescription because by decreasing $c_i$ proportionally for all points means that the scale of $D_{:, i} can be increased. To give the problem a unique solution, the columns of $D$ are constrained to have unit norm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to turn this idea into an algorithm, a first step required to figure out how to generate the optimal code point $c^*$ for each input point $x$. One way to do this is to minimize the distance between the input point $x$ and its reconstruction, $g(c*)$. This distance can be measured using a norm; in this case the $L^2$ norm is used:\n",
    "\n",
    "\\begin{equation}\n",
    "c^* = arg min_{c}||x - g(c)||_2\n",
    "\\end{equation}\n",
    "\n",
    "The squared $L^2$ norm can be used as both are minimized by the same value of $c$ as the $L^2$ norm is non-negative and the squaring operation is monotonically increasing for non-negative arguments:\n",
    "\n",
    "\\begin{equation}\n",
    "c^* = arg min_{c}||x - g(c)||_2^2\n",
    "\\end{equation}\n",
    "\n",
    "This function being minimized simplifies to \n",
    "\n",
    "\\begin{equation}\n",
    "(x - g(c))^T(x-g(c))\n",
    "\\end{equation}\n",
    "\n",
    "Expanding the expression,\n",
    "\n",
    "\\begin{equation}\n",
    "x^Tx - x^Tg(c) - g(c)^Tx + g(c)^Tg(c)\n",
    "\\end{equation}\n",
    "\n",
    "Using the distributive property (because the scalar g(c)^Tx is equal to the transpose of itself),\n",
    "\\begin{equation}\n",
    "x^Tx - 2x^Tg(c) + g(c)^Tg(c)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now the function being minimized can be rewritten to omit the first term as it does not depend on $c$:\n",
    "\n",
    "\\begin{equation}\n",
    "c^* = argmin_c -2x^TDc + c^TD^TDc\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_c -2x^TDc + c^TI_lc\n",
    "\\end{equation}\n",
    "\n",
    "(by the orthogonality constaints on $D$)\n",
    "\n",
    "so \n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_c -2x^TDc + c^Tc\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimization problem can be solved using vector calculus (figure out this step...):\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_c(-2x^TDc +c^Tc) = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "-2D^Tx + 2c = 0\n",
    "\\end{equation}\n",
    "\n",
    "so \n",
    "\n",
    "\\begin{equation}\n",
    "c = D^Tx\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this algorthim efficient: $x$ can be optimally encoded using just a matrix-vector operation:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = D^Tx\n",
    "\\end{equation}\n",
    "\n",
    "Using a further matrix multiplication, we can define the PCA reconstruction operation:\n",
    "\n",
    "\\begin{equation}\n",
    "r(x) = g(f(x)) = DD^Tx\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to choose the encoding matrix $D$ using the $L^2$ distance between inputs and reconstructions. Since we will use the same matrix $D$ to decode all the points, we can no longer consider the points in isolation. Instead, the Frobenius norm of the matrix of errors computed over all dimensions and all points must be minimized:\n",
    "\n",
    "\\begin{equation}\n",
    "D^* = argmin_D \\sqrt{\\sum_{i, j}(x_j^{(i)} - r(x^{i}_j))^2}\n",
    "\\end{equation}\n",
    "\n",
    "(Subject to $D^TD = I_l$)\n",
    "\n",
    "To derive the algorithm for finding $D*$, first consider the case where $l=1$. In this case, $D$ is just a single vector, $d$. Using the expression for the reconstruction in the Frobenius norm calculation gives:\n",
    "\n",
    "\\begin{equation}\n",
    "d^* = argmin_d \\sum_i ||x^{(i)} - dd^Tx^{(i)}||_2^2\n",
    "\\end{equation}\n",
    "\n",
    "(Subject to $||d||_2 = 1$)\n",
    "\n",
    "Or, exploiting the fact that a scalar is its own transpose:\n",
    "\n",
    "\\begin{equation}\n",
    "d^* = argmin_d \\sum_i ||x^{(i)} - x^{(i)}dd||_2^2\n",
    "\\end{equation}\n",
    "\n",
    "(Again, subject to $||d||_2 = 1$)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is most helpful to rewrite the problem in terms of a simple design matrix of examples. rather than as a sum over separate example vectors which enable the use of more compact notation.\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{m \\times n} be the matrix defined by stacking all the of the vectors describing the points, such that $X_{i, :} = x^{(i)T}$. The problem can be rewritten as:\n",
    "\n",
    "\\begin{equation}\n",
    "d^* = argmin_d ||X - Xdd^T||^2_F\n",
    "\\end{equation}\n",
    "\n",
    "(Subject to $d^Td = 1$)\n",
    "\n",
    "Disregarding the constraint for the moment, the Frobenius norm portion can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "argmin_d ||X - Xdd^T||^2_F\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_d \\ Tr((X - Xdd^T)^T(X - Xdd^T))\n",
    "\\end{equation}\n",
    "\n",
    "(Using the above expression for the Frobenius norm written in terms of the trace).\n",
    "\n",
    "\n",
    "Expanding the above expression,\n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_d \\ Tr(X^TX - X^TXdd^T - dd^TX^TX + dd^TX^TXdd^T)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_d \\ Tr(X^TX) - Tr(X^TXdd^T) - Tr(dd^TX^TX) + Tr(dd^TX^TXdd^T)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_d \\ - Tr(X^TXdd^T) - Tr(dd^TX^TX) + Tr(dd^TX^TXdd^T)\n",
    "\\end{equation}\n",
    "\n",
    "(As  Tr(X^TX) does not depend on d and so does not effect the argmin)\n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_d \\ -2Tr(X^TXdd^T) + Tr(dd^TX^TXdd^T)\n",
    "\\end{equation}\n",
    "\n",
    "(As the order of the matrices can be cycled inside the trace operator and so again, )\n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_d \\ -2Tr(X^TXdd^T) + Tr(X^TXdd^Tdd^T)\n",
    "\\end{equation}\n",
    "\n",
    "Reintroducing the constraint that $dd^T = 1$ then:\n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_d \\ -2Tr(X^TXdd^T) + Tr(X^TXdd^T)\n",
    "\\end{equation}\n",
    "\n",
    "so \n",
    "\n",
    "\\begin{equation}\n",
    "= argmin_d \\ -Tr(X^TXdd^T)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "= argmax_d \\ Tr(X^TXdd^T)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "d^* = argmax_d \\ Tr(d^TX^TXd)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This optimization problem may be solved by eigendecomposition. Specifically, the optimal $d$ is given by the eigenvector of $X^TX$ corresponding to the largest eigenvalue. This derivation is specific to the case of $l = 1$ and recovers only the first principal component, More generally, when a basis of principal components is desired, the matrix $D$ is given by the $l$ eigenvectors corresponding to the largest eigenvalues. This may be shown by proof by induction. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

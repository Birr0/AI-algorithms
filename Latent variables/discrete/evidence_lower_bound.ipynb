{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence Lower Bound (ELBO)\n",
    "\n",
    "An even more general perspective on the EM algorithm is to consider the evidence lower bound (ELBO). It also sometimes refered to as the variational lower bound. \n",
    "\n",
    "The evidence refers to the 'model evidence' in a Bayesian setting - it allows different models to be compared without the use of hold-out data. \n",
    "\n",
    "The ELBO is also a demonstration of a variation framework in which a distribution is introduced over the latent variables which is subsequently optimized with respect to the distribution using the calculus of variations. \n",
    "\n",
    "Again the goal is to maximize the log likelihood of the data:\n",
    "\n",
    "\\begin{equation}\n",
    "p(X|\\theta) = \\sum_{Z} p(X, Z|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Assuming $z$ is a discrete latent variable. This is valid for continuous latent variables with replacement of the summation with an integral. \n",
    "\n",
    "Suppose that direct optimization of $p(X|\\theta)$ is difficult but that optimization of the complete-data likelihood $p(X, Z|\\theta)$ is much easier. Introducing a distribution $q(Z)$ over the latent variables, the log likelihood can be decomposed into:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln p(X|\\theta) = \\mathcal{L}(q, \\theta) + KL(q||p)\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(q, \\theta) = \\sum_{Z} q(Z) \\ln \\left( \\frac{p(X, Z|\\theta)}{q(Z)} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{equation}\n",
    "KL(q||p) = - \\sum_{Z} q(Z) \\ln \\left( \\frac{p(Z|X, \\theta)}{q(Z)} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "is the Kullback-Leibler divergence between the variational distribution $q(Z)$ and the posterior of the latent variables $p(Z|X, \\theta)$. \n",
    "\n",
    "To verify this decompositon, the product rule gives:\n",
    "\n",
    "\\begin{equation}\n",
    "p(X, Z|\\theta) = p(Z|X, \\theta) p(X|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "and so \n",
    "\n",
    "\\begin {equation}\n",
    "\\ln p(X, Z|\\theta) = \\ln p(Z|X, \\theta) + \\ln p(X|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Substituting this into the expression for $\\mathcal{L}(q, \\theta)$ and noting that $\\sum_{Z} q(Z) = 1$ verifies the decomposition. \n",
    "\n",
    "Recall the the KL divergence satisifies $KL(q||p) \\geq 0$ with $KL(q||p) = 0$ if and only if $q(Z) = p(Z|X, \\theta)$. It follows then that $\\mathcal{L}(q, \\theta) \\leq \\ln p(X|\\theta)$ so that $\\mathcal{L}(q, \\theta)$ provides a lower bound to the log likelihood.\n",
    "\n",
    "![ELBO](./figures/ELBO.png)\n",
    "\n",
    "This decompostion can be used to derive the EM algorithm and verify that it does maximize the log likelihood. The current value of the parameter vector is $\\theta^{old}$. In the E-step, the lower bound is maximized with respect to $q(Z)$ while holding $\\theta^{old}$ fixed. As $\\ln p(X| \\theta^{old})$ does not depend on $q(Z)$, the largest value of $\\mathcal{L}(q, \\theta^{old})$ is obtained when $KL(q||p) = 0$, which occurs when $q(Z) = p(Z|X, \\theta^{old})$. In this case, the lower bound will equal the log likelihood. \n",
    "\n",
    "In the M-step, the distribution $q(Z)$ is held fixed and the lower bound is maximized with respect to $theta$ to provide some new parameter values $\\theta^{new}$. This will cause the lower bound to increase, unless it is already at a maximum.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization algorithm \n",
    "\n",
    "Denote the set of N observed data points by the matrix $X$, in which the $n$ th row is the $x_n^T$. The corresponding latent variables will be denoted by an $N \\times K$ matrix $Z$ with rows $z_n^T$ where $K$ is the dimensionality of the latent space.\n",
    "\n",
    "The set of all model parameters can be denotes by $\\theta$ and the log-likelihood function is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln p(X|\\theta) = \\ln \\left( \\sum_{Z} p(X, Z|\\theta) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "*Note: this applies well to continuous latent variables simply by replacing the sum over $Z$ with an integral.*\n",
    "\n",
    "The presence of the sum over the latent variables ($Z$) prevents the logarithm from acting directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution. \n",
    "\n",
    "$\\{ X, Z \\}$ is called the *complete data* and $\\{ X \\}$ is the *incomplete data*. \n",
    "\n",
    " Suppose that the maximization of the complete data log-likelihood is straightforward. In practice the data is incomplete and so we consider instead its expected value under the posterior distribution of the latent variables and our knowledge of the latent variables is given only by this posterior. This corresponds to the E-step of the EM algorithm. In the subsequent M-step, the expectation is maximized and the previous parameters, $\\theta^{old}$, are updated to $\\theta^{new}$. The algorithm is initialized by choosing an initial parameter $\\theta_0$. \n",
    "\n",
    "In the E step, we use the current parameter values $\\theta^{old}$ to find the posterior distribution of the latent variables given by $p(Z|X, \\theta^{old})$. We then use this posterior distribution find the expectation of the complete-data log-likelihood evaluate for some general parameter value $\\theta$. This expectations is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{Q}(\\theta, \\theta^{old}) = \\sum_{Z} p(Z|X, \\theta^{old}) \\ln p(X, Z|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "In the M-step, we determine the revised parameter estimate $\\theta^{new}$ by maximizing this expected log-likelihood over $\\theta$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^{new} = \\arg \\max_{\\theta} \\mathcal{Q}(\\theta, \\theta^{old})\n",
    "\\end{equation}\n",
    "\n",
    "The general EM, given below, has the property that each cycle of the EM will increase the incomplete-data log-likelihood, unless it is already at a local maximum.\n",
    "\n",
    "# \n",
    "# ![General EM Algorithm](./figures/general_EM.png)\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General EM algorithm\n",
    "\n",
    "Input:\n",
    "    Joint distribution $p(X, Z|\\theta)$\n",
    "    Initial parameters $\\theta^{\\text{old}}$\n",
    "    Data set $\\{x_1, \\dots, x_N \\}$\n",
    "\n",
    "Output:\n",
    "    Final parameters $\\theta$\n",
    "\n",
    "\n",
    "**repeat** \n",
    "\n",
    "E-step\n",
    "\n",
    "$\\mathcal{Q}(\\theta, \\theta^{\\text{old}}) \\leftarrow \\sum_z p(Z|X, \\theta^{\\text{old}}) \\ln p(X, Z|\\theta)$ \n",
    "\n",
    "(This is the expectation of log-likehood evauluated for a general value of $\\theta$ using the posterior of the latent variables $Z$ given $X$ and $\\theta$.)\n",
    "\n",
    "M-step - find the maximum of the expectation of the log-likelihood\n",
    "\n",
    "$\\theta^{\\text{new}} \\leftarrow \\arg \\max_{\\theta} \\mathcal{Q}(\\theta, \\theta^{\\text{old}})$\n",
    "\n",
    "$\\mathcal{L} \\leftarrow p(X|\\theta^{\\text{new}})$ - Evaluate log likelihood\n",
    "\n",
    "$\\theta^{\\text{old}} \\leftarrow \\theta^{\\text{new}}$ \n",
    "\n",
    "**until** convergence \n",
    "\n",
    "**return** $\\theta^{\\text{new}}$\n",
    "\n",
    "Note: the inclusion of a prior over the model parameters makes the function to be maximised in the M-step to be\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{Q}(\\theta, \\theta^{\\text{old}}) + \\ln p(\\theta)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixtures \n",
    "\n",
    "The goal is to maximise the log likelihood:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln p(X|\\pi, \\mu, \\Sigma) = \\sum_{n=1}^{N} \\ln \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "which is made difficult by the sum over the latent vairable inside the logarithm.\n",
    "\n",
    "Suppose in addition to the observed data set, we have also access to the corresponding discrete variables $Z$. Maximising the likelihood of the complete data (i.e. $X$ and $Z$) is\n",
    "\n",
    "\\begin{equation}\n",
    "p(X, Z| \\mu, \\Sigma, \\pi) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\pi_k^{z_{nk}} \\mathcal{N}(x_n | \\mu_k, \\Sigma_k))^{z_{nk}}\n",
    "\\end{equation}\n",
    "\n",
    "where $z_{nk}$ denotes the $k$ th component $z_n$. Taking the logarithm, we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln p(X, Z| \\mu, \\Sigma, \\pi) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_nk \\{ \\ln \\pi_k + \\ln \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\}\n",
    "\\end{equation}\n",
    "\n",
    "The summation and the logarithm have been interchanged and therefore the logarithm acts directly on the Gaussian distribution which is a member of the exponential family. This leads to a much more straightforward solution to the maximum likelihood problem. \n",
    "\n",
    "\n",
    "\n",
    "For example, consider the maximization of the mean or covariance matrices. As $z_n$ is a $K-$dimensional vector with all elements equal to zero except for a single element having a value of one, the complete-data log-likelihood is simply a sum of $K$ independent contributions, one from each mixture component. Therefore the maximization with respect to a mean or covariance is exactly as in the case of a single Gaussian distributions, except that it involves only the subset of the data points that are assigned to the corresponding component. \n",
    "\n",
    "For the maximization of the mixing coefficients, note that these are coupled for different values of $k$ by virtue of the summation constraint, $\\sum_{k} \\pi_k = 1$. This can be enforced using a Lagrange multiplier as before which leads to the result:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_k = \\frac{1}{N} \\sum_{n=1}^{N} z_{nk}\n",
    "\\end{equation}\n",
    "\n",
    "The mixing coefficeints are equal to the fractions of data points assigned to the corresponding components.\n",
    "\n",
    "The complete-data log-likelihood can be maximzed in closed form. Howeverm in practice we don't have access to the latent variables $Z$. Instead, as above, consider instead the expectation with repsect to the posterior distribution of the latent variables of the complete-data log-likelihood. \n",
    "\n",
    "As the marginal distribution over z is\n",
    "\n",
    "\\begin{equation}\n",
    "p(z) = \\prod_{k=1}^{K} \\pi_k^{z_k}\n",
    "\\end{equation}\n",
    "\n",
    "and the conditional distribution of the data given the data and the latent variables is\n",
    "\n",
    "\\begin{equation}\n",
    "p(x|z) = \\prod_{k=1}^{K} \\mathcal{N}(x | \\mu_k, \\Sigma_k)^{z_k}\n",
    "\\end{equation}\n",
    "\n",
    "Using Bayes theorem, we can express the posterior distribution of the latent variables as:\n",
    "\n",
    "\\begin{equation}\n",
    "p(Z| X, \\mu, \\Sigma, \\pi) \\propto \\prod_{n=1}^{N} \\prod_{k=1}^{K} [\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)]^{z_{nk}}\n",
    "\\end{equation}\n",
    "\n",
    "It can be seen that this factorizes over $n$ so that under the posterior distribution, $\\{ z_n \\}$ are independent. \n",
    "\n",
    "The expected value of the indicator variable (i.e. the assigment cluster) is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[z_{nk}] = \\frac{\\sum_{z_n} z_{nk} \\prod_{k'} [\\pi_k' \\mathcal{N}(x_n | \\mu_k', \\Sigma_k')]^{z_{nk'}}}{\\sum_{z_n} \\prod_{j} [\\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)]^{z_{nj}}}\n",
    "\\end{equation}\n",
    "\n",
    "I don't get the step in Bishop pg. 479 to get what follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[z_{nk}] = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)} = \\gamma(z_{nk})\n",
    "\\end{equation}\n",
    "\n",
    "which is the responsibility of component $k$ for data point $n$. The expected value of the complete-data log-likelihood is then given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[\\ln p(X, Z | \\mu, \\Sigma, \\pi)] = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\gamma(z_{nk}) \\{ \\ln \\pi_k + \\ln \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\}\n",
    "\\end{equation}\n",
    "\n",
    "As the E-step has been defined, some initial values for the parameters $\\mu^{old}, \\Sigma^{old}, \\pi^{old}$ are choosen and used to evaluate the responsibilites in the E-step. Then the responsbilites are fixed and the above is maximised with respect to $\\mu, \\Sigma, \\pi$ in the M-step. This is repeated until convergence is reached. \n",
    "\n",
    "Note: This assumes that the data observations are i.i.d. For ordered observations that form a sequence, the mixture model can be extended by connecting the latent variables in a Markov chain to give a Hidden Markov Model (HMM).\n",
    "\n",
    "There is a relationship between the Gaussian mixture model and the K-means, especially when compared using the EM algorithm. Where as the K-means algorithm performs a hard assignment of data points to clusters, the Gaussian mixture model allows for a soft assignment based on the posterior probability of the latent variables. \n",
    "\n",
    "Consider a GMM in which the covariance matrices are given by $\\epsilon I$, where $\\epsilon$ is a variance parameter shared by all components so the likelihood function for a given cluster $k$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "p(X|\\mu_k, \\Sigma_k) = \\frac{1}{(2 \\pi \\epsilon)^{D/2}} \\exp \\left( -\\frac{||x - \\mu_k||^2}{2 \\epsilon} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Treating $\\epsilon$ as a constant, the posterior probabilites or responsibilities are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\gamma(z_{nk}) = \\frac{\\pi_k exp \\{ -||x_n - \\mu_k||^2 / 2\\epsilon \\}}{\\sum_j \\pi_j exp \\{ -||x_n - \\mu_j||^2 / 2\\epsilon \\}}\n",
    "\\end{equation}\n",
    "\n",
    "In the limit $\\epsilon \\to 0$, the dominator is dominated the terms for which $||x_n - \\mu_k||^2$ is smallest. Therefore, the responsibility of this cluster will go to unity as in the case of the K-means algorithm. Furthermore, the expected complete-data log-likelihood becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[\\ln p(X, Z|\\mu, \\Sigma, \\pi)] = -\\frac{1}{2} \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} ||x_n - \\mu_k||^2 + \\text{const}\n",
    "\\end{equation}\n",
    "\n",
    "In this limit, maximizing the expected complete-data log likelihood is equivalent to minimizing the error measure $J$ for the K-means algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement LayerNorm layer\n",
    "# MLP layer\n",
    "# Self-attention layer (decoder)\n",
    "# Casual/masked self-attention layer (encoder)\n",
    "\n",
    "# Transformer block (inlcude layer norm, MLP, and self-attention)\n",
    "\n",
    "# Decoder (consisting of blocks, mlps and residual connections)\n",
    "# Encoder (consisting of blocks, mlps and residual connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Usage\n",
    "set_seed(42)  # Or any integer of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm \n",
    "\n",
    "This applies layer normalization over the input (minibatch) of inputs, given by the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}}*\\gamma + \\beta\n",
    "\\end{equation}\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learnable parameters. The mean and variance are computed over the last $D$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Example of LayerNorm\n",
    "\n",
    "batch_size = 10 # Number of examples in the batch\n",
    "sequence_length = 5 # Number of tokens in the sequence\n",
    "embedding_dim = 3 # Dimensionality of each token\n",
    "\n",
    "# Generate random inputs for a single batch \n",
    "x = torch.randn(batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "layer_norm = LayerNorm(embedding_dim, bias=True)\n",
    "output = layer_norm(x)\n",
    "manual_output = ((x - x.mean(-1).unsqueeze(-1))/(torch.sqrt(x.var(-1, unbiased=False).unsqueeze(-1)) + 1e-5)) #torch.round((x - x.mean(-1).unsqueeze(-1))/(torch.sqrt(x.var(-1, unbiased=False).unsqueeze(-1)) + 1e-5), decimals=2) * torch.tensor(layer_norm.weight).unsqueeze(-1) * torch.scalar_tensor(1.0) + torch.scalar_tensor(0.0)\n",
    "\n",
    "# Something weird with numerical precision here but the values are close...\n",
    "\n",
    "print(torch.allclose(torch.round(output, decimals=2), torch.round(manual_output, decimals=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casual Self-Attention (aka masked SA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The casual self-attention mask looks like this:\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "In order to apply this mask to the attention weights, we need to expand the mask to the batch size and number of heads so that it can be broadcasted over the sequence and batch.\n",
      "torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"The casual self-attention mask looks like this:\")\n",
    "block_size = 5\n",
    "print(casual_mask := torch.tril(torch.ones((block_size, block_size))))\n",
    "\n",
    "print(\"In order to apply this mask to the attention weights, we need to expand the mask to the batch size and number of heads so that it can be broadcasted over the sequence and batch.\")\n",
    "print(casual_mask:= casual_mask.view(1, 1, block_size, block_size).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.embedding_dim % config.num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        # Input layer. Calculate Q, K and V.\n",
    "        self.attn = nn.Linear(config.embedding_dim, 3 * config.embedding_dim, bias=config.bias)\n",
    "        # Output layer projection\n",
    "        self.attn_output = nn.Linear(config.embedding_dim, config.embedding_dim, bias=config.bias)\n",
    "\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout_rate)\n",
    "        self.residual_dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.embedding_dim = config.embedding_dim\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: Flash Attention not available, using PyTorch implementation of slow attention.\")\n",
    "        self.layer_norm = LayerNorm(config.embedding_dim, config.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # B - batch size, T - sequence length, C - embedding dimension \n",
    "\n",
    "        # Calculate the Queries, Keys and Values \n",
    "        q, k, v = self.attn(x).split(self.embedding_dim, dim=2)\n",
    "\n",
    "        # Project the queries, keys and values to the shape (B, num_heads, T, head_size)\n",
    "        # C = note num_heads*head_size \n",
    "        # This splits the embedding dimension equally across the number of attention heads. \n",
    "\n",
    "        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # att_mask here is important for ignoring elements that should be masked or padded.\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout_rate if self.training else 0, is_causal=False)\n",
    "        else:\n",
    "            # Implement self-attention manually\n",
    "            (B, num_heads, T, head_size)* (B, num_heads, T, head_size)\n",
    "            # Need to transpsoe k from (B, num_heads, T, head_size) to (B, num_heads, head_size, T) which when multipled by q gives (B, num_heads, T, T)\n",
    "            attn = (q @ k.transpose(-2, -1) * (1.0 / math.sqrt(C // self.num_heads)))\n",
    "            attn = F.softmax(attn, dim=-1) # Compute SM across the seqeunce \n",
    "            attn = self.attn_dropout(attn)\n",
    "            y = attn @ v\n",
    "        \n",
    "        # Reassemble y \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # reasseble y into the original shape \n",
    "        y = self.residual_dropout(self.attn_dropout(y)) # Apply dropout to attn and dropout. Project head to output.\n",
    "        return y\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''\n",
    "        config: a dictionary containing the following parameters:\n",
    "            - block_size: the number of tokens in the sequence\n",
    "            - embedding_dim: the dimensionality of the input embeddings\n",
    "            - num_heads: the number of attention heads\n",
    "            - dropout_rate: the dropout rate for the attention weights\n",
    "            - bias: whether to use bias in the linear layers\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert config.embedding_dim % config.num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        # Calculate the Queries, Keys, and Values\n",
    "        self.casual_attn = nn.Linear(config.embedding_dim, 3 * config.embedding_dim, bias=config.bias)\n",
    "\n",
    "        # Output layer projection \n",
    "        self.casual_attn_output = nn.Linear(config.embedding_dim, config.embedding_dim, bias=config.bias)\n",
    "\n",
    "        #Â Regularization\n",
    "        self.casual_attn_dropout = nn.Dropout(config.dropout_rate)\n",
    "        self.residual_dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "        self.num_heads = config.num_heads\n",
    "        self.embedding_dim = config.embedding_dim\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: Flash Attention not available, using PyTorch implementation of slow attention.\")\n",
    "            # Add the casual (triangular) mask to ensure attention is only applied to tokens previously seen in the sequence.\n",
    "            # Use a buffer to ensure the mask is stored as a buffer in the module, not as a parameter. \n",
    "            # Buffers are not updated during training, are saved with the model state and moved to the device. \n",
    "            # Commonly used for constants, caching computed values and maintaining statistics.\n",
    "   \n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones((config.block_size, config.block_size))).view(1, 1, block_size, block_size))\n",
    "        # Layer normalization\n",
    "        self.layer_norm = LayerNorm(config.embedding_dim, config.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # B - batch size, T - sequence length, C - embedding dimension\n",
    "\n",
    "        # calculate the queries, keys, and values\n",
    "        q, k, v = self.casual_attn(x).split(self.embedding_dim, dim=2) # split the output of the linear network into q,k and v.\n",
    "\n",
    "        # project each of the queries, keys, and values to:\n",
    "        # the shape (B, num_heads, T, head_size) i.e. split the embedding dimensions from \n",
    "        # q,k and v equally across the number of attention heads in a block. (head_size = embedding_dim / num_heads)\n",
    "        # Then transpose the dimensions so that the sequence length is first.\n",
    "    \n",
    "        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) \n",
    "        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout_rate if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # Implement causal self-attention manually\n",
    "            # q = (B, num_heads, T, head_size), k = (B, num_heads, T, head_size), k.transpose(-2, -1) -> (B, num_heads, head_size, T) - swapping the last two dimensions enables broadcasting\n",
    "\n",
    "            attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(C // self.num_heads)) # scaled dot product self-attention (d_k is the square root of the head size)\n",
    "\n",
    "            # Apply the softmax\n",
    "\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "            # Apply dropout to the attention weights    \n",
    "        \n",
    "            attn = self.casual_attn_dropout(attn)\n",
    "\n",
    "            # Compute the output\n",
    "            y = attn @ v # (B, num_heads, T, T) x (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Reassemble all of the head outputs (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, C)\n",
    "        y = self.residual_dropout(self.casual_attn_output(y)) # Project the heads to the output and \n",
    "        return y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__() \n",
    "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias)\n",
    "        self.gelu = nn.GELU() \n",
    "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x)) # residual connected attention layer\n",
    "        x = x + self.mlp(self.ln_2(x)) # residual connectioned mlp layer \n",
    "        return x    \n",
    "\n",
    "class CasualAttentionBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x)) # residual connection\n",
    "        x = x + self.mlp(self.ln_2(x)) # residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    block_size: int = 5\n",
    "    n_token: int = 6\n",
    "    n_embed: int = 3\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True\n",
    "\n",
    "# Dummy numbers for the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embed is not None \n",
    "        assert config.block_size is not None \n",
    "        self.config = config\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            tok_to_emb = nn.Embedding(config.n_token, config.n_embed),\n",
    "            pos_emb = nn.Embedding(config.block_size, config.n_embed),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([CasualAttentionBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        ))\n",
    "\n",
    "        # project back to the token space\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.n_token, bias=config.bias) \n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.decoder.tok_to_emb.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialize weights here...\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        for pn, p in self.named_parameters():\n",
    "            # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model. \n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        Token embeddings would be subtracted too, except due to the parameter sharing they are actually \n",
    "        used as weights in the final layer, so they are included.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.decoder.pos_emb.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()      \n",
    "        \n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # position indices\n",
    "\n",
    "        # forward the model \n",
    "        tok_emb = self.decoder.tok_to_emb(idx) # token embedding of shape - (b, t, embed_dim)\n",
    "        pos_emb = self.decoder.pos_emb(pos) # position embedding of shape - (t, n_embed)\n",
    "\n",
    "        x = self.decoder.drop(tok_emb + pos_emb)\n",
    "\n",
    "        for block in self.decoder.h:\n",
    "            x = block(x)\n",
    "        x = self.decoder.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # calculate the loss given some targets \n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference mode. Mini-optimization: only forward the lm_head on the last position \n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None \n",
    "        return logits, loss \n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary \n",
    "        assert block_size <= self.config.block_size \n",
    "        self.config.block_size = block_size \n",
    "        self.decoder.tok_emb.weight = nn.Parameter(self.decoder.tok_emb.weight[:block_size])\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
    "\n",
    "\n",
    "        for block in self.decoder.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out params that don't require grad\n",
    "        param_dict = {pn:p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optimizer groups. Any parameters that are 2D will be weight decayed.\n",
    "        # This includes all weight tensors in matmuls + embeddings. All biases and layernorms will not be weight decayed.\n",
    "\n",
    "        decay_params = [p for n,p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n,p in param_dict.items() if p.dim() < 2]\n",
    "\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "        print(f\"num decayed parameter tensors: {len(optim_groups[0]['params'])}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(optim_groups[1]['params'])}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        # Create AdamW optimizer and use the fused version if it's available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"Using fused AdamW: {use_fused}\")\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embed is not None \n",
    "        assert config.block_size is not None \n",
    "\n",
    "        self.config = config \n",
    "\n",
    "        self.encoder = nn.ModuleDict(dict(\n",
    "            tok_to_emb = nn.Embedding(config.n_token, config.n_embed),\n",
    "            pos_emb = nn.Embedding(config.block_size, config.n_embed),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([SelfAttentionBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        ))\n",
    "\n",
    "        # Project output back to the token space - replace with flux + error prediction ...\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.n_token, bias=config.bias)\n",
    "        \n",
    "        # Weight tying \n",
    "        self.encoder.tok_to_emb.weight = self.lm_head.weight\n",
    "\n",
    "        # Init weights\n",
    "        # ...\n",
    "\n",
    "    def get_num_params(self, non_embedding):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model. \n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            # Subtract positional embeddings \n",
    "            n_params -= self.encoder.pos_emb.weight.numel()\n",
    "        return n_params \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear): \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size() # check the size of the device\n",
    "\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\" \n",
    "\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # position indices \n",
    "\n",
    "        # forward the model \n",
    "        tok_emb = self.encoder.tok_to_emb(idx) # token embeddings \n",
    "        pos_emb = self.encoder.pos_emb(pos) # position embeddings \n",
    "\n",
    "        x = self.encoder.drop(tok_emb + pos_emb) # drop tok_emb and pos_emb \n",
    "\n",
    "        for block in self.encoder.h:\n",
    "            x = block(x)\n",
    "        x = self.encoder.ln_f(x)\n",
    "\n",
    "        if targets is not None: # Training mode\n",
    "            # Change to match the desired output...\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        else: # Inference mode\n",
    "            logits = self.lm_heead(x[:, [-1], :]) # Preserve the time dimension with [-1]\n",
    "            loss = None \n",
    "        return logits, loss \n",
    "\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgey to decrease block size if necessary \n",
    "        assert block_size <= self.config.block_size \n",
    "        self.config.block_size = block_size \n",
    "        \n",
    "        self.encoder.tok_emb.weight = nn.Parameter(self.encoder.tok_emb.weight[:block_size])\n",
    "\n",
    "        for block in self.encoder.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out params that don't require grad \n",
    "        param_dict = {pn:p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # Create optimizer groups for decay and non-decay params \n",
    "\n",
    "        decay_params = [p for n,p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "        print(f\"num decayed parameter tensors: {len(optim_groups[0]['params'])}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(optim_groups[1]['params'])}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        # Create AdamW optimizer and use the fused version if available for efficiency\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters \n",
    "        use_fused = fused_available and device_type == 'cuda' \n",
    "        extra_args = dict(fused=True) if use_fused else dict() \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"Using fused AdamW: {use_fused}\")\n",
    "        return optimizer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

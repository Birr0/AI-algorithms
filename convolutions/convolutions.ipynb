{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks \n",
    "\n",
    "\n",
    "For data arranged in a grid, such as an image, points in close to each other tend to have highly correlated values. This is powerful prior knowledge of the data and can be used to encode strong inductive biases into the network, leading to models with fewer parameters and better generalization accuracy. An architectural approach to grid data is the convolutional neural network, which by using filters, is designed to encode invariances and equivariances specific to image-like data. A CNN can be viewed as a sparsely connected multilayer network with parameters sharing.\n",
    "\n",
    "\n",
    "* Invariant: a property of an object which remains unchaged after operations or transformations of a certain type.\n",
    "\n",
    "* Equivariance: a form of symmetry for functions from one space with symmetry to another. (i.e. applying a symmetry transformation and then computing the function produces the same result as computing the function and then applying the transformation.)\n",
    "\n",
    "To exploit the two-dimensional structure of image data to create strong inductive biases, four interrelated concepts can be applied: hierarchy, locality, equivariance and invariance.\n",
    "\n",
    "Consider a single unit in the first layer of a neural network that takes as input the pixel values from a small rectangular region or patch of the image. This patch is known as the receptive field of that unit and it captures the local structure. By assigning weight values associated with this unit, this unit can learn useful low-level features. The output of this unit is given by the usual functional form of a weighted linear combination of the input values, tranformed by a non-linearity:\n",
    "\n",
    "\\begin{equation}\n",
    "z = ReLU(\\textbf{w}^T \\textbf{x} + w_0)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\textbf{w}$ and $w_0$ are the weights and bias, respectively and $\\textbf{x}$ is a vector of pixel values for the receptive field. As there is on eweight associated with each input pixel, the weights themselves form a small two-dimensional grid known as a filter or kernel, which can also be visulaized as an image:\n",
    "\n",
    "# ![Receptive Field - Bishop pg. 291](./figures/receptive_field.png)\n",
    "\n",
    "The solution for $x$ that maximizes the response $w^T x + w_0$ assuming the weights and biases are fixed and that the size of $x$ is normalized is in the form $x = \\alpha w$. This is effectively saying that that the maximum response will be obtained when the filter detects a patch that looks like the filter itself. This is a feature detector when $w^Tx > -w_0$. The units of the the hidden layer form a feature map in which all the units share the same weights. If a local patch of an image produces a particular response in the unit connected to that patch, then the same set of pixel values at a different location will produce the same response in the corresponding translated location in the feature map. This is one example of (translational) equivariance.\n",
    "\n",
    "A convolution of an image $I(j \\times k)$ with a filter $K(l \\times m)$ is given as follows (omitting the non-linearity):\n",
    "\n",
    "\\begin{equation}\n",
    "C = I * K = C(j, k) = \\sum_l \\sum_m I(j+l, k+m)K(l, m)\n",
    "\\end{equation}\n",
    "\n",
    "This strictly speaking is a cross-correlation, not a convolution.\n",
    "\n",
    "# ![Convolution](./figures/convolution.png)\n",
    "\n",
    "If the image ($J \\times K$) is convolved with a filter ($M \\times M$), the resulting feature map will be of dimension $(J-M+1) \\times (K-M+1)$.\n",
    "\n",
    "### Padding\n",
    "In cases where we would like the feature map to be the same size as the input, padding can be used. This means that pixels are added around the outside. For padding of $P$ pixels, the output map has dimension $(J+2P-M+1) \\times (K+2P-M+1)$. $P=0$ is known as a *valid* convolution, while $P=(M-1)/2$ is known as a *same* convolution. Usually, the padding values are set to zero. Padding can also be applied to feature maps in deeper layers, not just input images.\n",
    "\n",
    "### Stride \n",
    "\n",
    "Typicallym images are large and filters are small such that $M << J, K$. The feature map then is of similar dimensionality to the original image and may be identical if padding is used. If feature maps that are significantly smaller than the input is required, strides can be applied. This means that the filter is moved across the image in steps of $S$ pixels. The resulting feature map then has dimension $\\lfloor (J-M+2P)/S - 1 \\rfloor \\times \\lfloor (K-M+2P)/S - 1\\rfloor$. ($\\lfloor$ denotes the floor function, the largest integer less than or equal to the quantity in the brackets). For large images and small filters, the image map will be roughly a factor of $1/S$ smaller than the image. \n",
    "\n",
    "### Multi-dimensional convolutions \n",
    "\n",
    "Typicall each pixel in an image has three values $(R, G, B)$ that represent the colour of the pixel. These are known as channels. Convolutions can be extended to to cover multiple channels by extedning the dimensionality of the filter. An image, represented by a tensor of dimensions $J \\times K \\times C$ can be convolved with a filter of dimensions $M \\times M \\times C$ to produce a feature map of dimensions $(J-M+1) \\times (K-M+1) \\times C$ using no padding and a stride of 1. This filter will have $M^2C$ weight parameters. Each filter is analogous to a single hidden node in a fully connected network and can be used to detect a single feature. Therefore, to build more flexible models, one must simply include more filters which each have their own set of learnable parameters giving rise to its own feature map. Each of these separate feature maps are also known as channels. The filter tensor now has dimensionality of $M \\times M \\times C \\times C_{OUT}$ where $C$ is the number of input channels and $C_{OUT}$ is the number of input channels. Each channel will have its own bias parameter so the total number of parameters will be $(M^2C + 1) C_{OUT}$. \n",
    "\n",
    "# ![Convolution over channels](./figures/convolution_over_channels.png)\n",
    "\n",
    "\n",
    "### Pooling (Down-sampling)\n",
    "\n",
    "In all forms of CNNs, a desired property is that the network can learn a hierarchical structure in which complex features at a particular level are built up from simplier features at a previous level. It is also important that learnt features are invariant to small translations of input features. Both desiderata can be achieved using pooling layers applied after convolutional layers. Along with building in some local translational invariance, pooling can also be used to reduce the dimensionality of the representation by down-sampling the feature map. (In a similar way that strides greater than one can in a convolutional layer). \n",
    "\n",
    "The output of a pooling unit is a simple fixed function of its inputs, and there are no learnable parameters in pooling. For example, max-pooling in which each unit simply outputs the $max$ function applied to the input values. Or, *average* pooling which the pooling function computes thr average of the values in the corresponding receptive field in the feature map. (All pooling functions introduce some degree of local translation invariance). \n",
    "\n",
    "# ![Max Pooling](./figures/max_pooling.png)\n",
    "\n",
    "The activation of a unit in a feature map can be interpretted as measire of the strength of detection of a corresponding feature, so that the max-pooling preserves information on whether the feature is present and with strength. However, it does discard some positional information.\n",
    "\n",
    "### Up-sampling\n",
    "\n",
    "The reverse of the down-sampling effects of strided convolutions and pooling. This can take the form of additional learnable layers that take low-dimensional internal representations and transform them back to the original image resolution. \n",
    "\n",
    "For example, the reverse of max-pooling is max-unpooling which assigns the max value to the first element of the block (this is rather arbitary therefore modified approaches exist for including where this max value occured in the feature map, preserving the spatial information from the down sampling layers). Average pooling follows a similar process and can be seen in the following diagram:\n",
    "\n",
    "# ![Unpooling](./figures/unpooling.png)\n",
    "\n",
    "\n",
    "### Fully convolutional networks \n",
    "\n",
    "The upsampling methods above are fixed functions. Learned up-sampling can also be used which is anlogous to strided convolution for down sampling. For up-sampling, we use a filter that connects one pixel in the input array to a patch in the output array, and then chose the architecture so that as we move one step across the input array, we move two or more steps across the output array.\n",
    "\n",
    "This is called *transpose convolution* because, when the down-sampling convolution is presented in matrix form, the corresponding up-sampling is given by the transpose matrix. It is also called 'fractionally strided convolution' because the stride of a standard convolution is the ratio of the step size in the output layer to the step size in the input layer. A network that uses no pooling layers, so that the down-sampling and up-sampling are done purley by convolutional layers is known as a *fully convolutional network*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example operations\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn \n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"MPS not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape is: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a batch of training data\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Display the first image in the batch\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.title(f'Label: {labels[0].item()}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"Image shape is: {images[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 3, kernel_size=(3, 3), stride=(2, 2))\n",
      "ConvTranspose2d(3, 1, kernel_size=(3, 3), stride=(2, 2))\n"
     ]
    }
   ],
   "source": [
    "conv2d_layer = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3,3), stride=2, padding=0)\n",
    "tconv2d_layer = nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=(3,3), stride=2, padding=0)\n",
    "\n",
    "# Number of out_channels is the number of filters/kernels - can learn more features...\n",
    "\n",
    "print(conv2d_layer)\n",
    "print(tconv2d_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature map from the Convolutional layer is : torch.Size([3, 13, 13])\n",
      "The shape of the feature map from the Convolutional layer is : torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Convolution and transpose convolution\n",
    "\n",
    "x = conv2d_layer(images[0]) \n",
    "\n",
    "print(f\"The shape of the feature map from the Convolutional layer is : {x.shape}\")\n",
    "print(f\"The shape of the feature map from the Convolutional layer is : {tconv2d_layer(x, output_size=images[0].size()).shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example training the MNIST for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature map from the Convolutional layer is : torch.Size([1, 10, 13, 13])\n",
      "The shape of the feature map from the pooling layer is : torch.Size([1, 10, 6, 6])\n",
      "The shape of the feature map from the pooling layer is : torch.Size([1, 360])\n"
     ]
    }
   ],
   "source": [
    "# Testing shapes \n",
    "\n",
    "conv2d_layer = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3,3), stride=2, padding=0)\n",
    "x = conv2d_layer(images[0:1]) \n",
    "print(f\"The shape of the feature map from the Convolutional layer is : {x.shape}\")\n",
    "#conv2d_layer = nn.Conv2d(in_channels=10, out_channels=5, kernel_size=(4,4), stride=2, padding=0)\n",
    "#x = conv2d_layer(x) \n",
    "#print(f\"The shape of the feature map from the Convolutional layer is : {x.shape}\")\n",
    "pooling = nn.MaxPool2d((2,2))\n",
    "x = pooling(x)\n",
    "print(f\"The shape of the feature map from the pooling layer is : {x.shape}\")\n",
    "flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "x = flatten(x)\n",
    "print(f\"The shape of the feature map from the pooling layer is : {x.shape}\")\n",
    "linear = nn.Linear(in_features=10*6*6, out_features=10)\n",
    "x = linear(x)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "x = softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'conv2d_layer1': nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5,5), stride=1, padding=2),\n",
    "            'pooling1': nn.AvgPool2d(kernel_size=(2,2), stride=2),\n",
    "            'sigmoid1': nn.Sigmoid(),\n",
    "            'conv2d_layer2': nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5,5), stride=1, padding=0),\n",
    "            'sigmoid2': nn.Sigmoid(),\n",
    "            'pooling2': nn.AvgPool2d(kernel_size=(2,2), stride=2),\n",
    "            'flatten': nn.Flatten(),\n",
    "            'linear1': nn.Linear(in_features = 5*5*16, out_features=120),\n",
    "            'sigmoid3': nn.Sigmoid(),\n",
    "            'linear2': nn.Linear(in_features = 120, out_features=84),\n",
    "            'sigmoid4': nn.Sigmoid(),\n",
    "            'linear3': nn.Linear(in_features = 84, out_features=10)\n",
    "        })\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (layers): ModuleDict(\n",
       "    (conv2d_layer1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (pooling1): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
       "    (sigmoid1): Sigmoid()\n",
       "    (conv2d_layer2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (sigmoid2): Sigmoid()\n",
       "    (pooling2): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear1): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (sigmoid3): Sigmoid()\n",
       "    (linear2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (sigmoid4): Sigmoid()\n",
       "    (linear3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - loss: 0.01712007237910739\n",
      "Epoch 2/10 - loss: 0.014794631013307073\n",
      "Epoch 3/10 - loss: 0.013038988124199381\n",
      "Epoch 4/10 - loss: 0.015535102003243933\n",
      "Epoch 5/10 - loss: 0.012860254407290377\n",
      "Epoch 6/10 - loss: 0.014383999507195202\n",
      "Epoch 7/10 - loss: 0.01356665266738554\n",
      "Epoch 8/10 - loss: 0.013556746040149762\n",
      "Epoch 9/10 - loss: 0.012912988099366866\n",
      "Epoch 10/10 - loss: 0.01485624890717958\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        #Â zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        outputs = model(images)\n",
    "\n",
    "        # loss \n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - loss: {running_loss/i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Accuracy on the test set: 98.93%\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# After your training loop, add this code to test the model\n",
    "print(\"Testing the model...\")\n",
    "test_accuracy = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics + visualisation - be a good time to code up classification metrics here...(Precision/recall/F1(macro/micro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an autoencoder - take Le-Net, use only the last layer, and reverse it to create a decoder. (Do this in the AE notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on VQ of the latent space - see how that effects accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the AE variational..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

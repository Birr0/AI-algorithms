{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    "Neural Discrete Representation Learning: de Oord et al. (2018)\n",
    "\n",
    "Understanding Vector Quantization in VQ-VAE: https://huggingface.co/blog/ariG23498/understand-vq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea of vector quantization is to quantize a continuous latent space into a discrete latent space. \n",
    "\n",
    "A latent embedding space is defined: $e \\in \\mathcal{R}^{K \\times D}$ where $K$ is the size of the discrete latent space. Effectively, each vector is a $K$-way categorical, and $D$ is the dimensionality of each latent embedding vector, $e_i$. There are $K$ embedding vectors $e_i \\in \\mathcal{R}^D, i \\in 1, 2,..., K$. \n",
    "\n",
    "The model takes an input $x$ and the encoder produces a continuous output $z_e(x)$. The discrete latent variables $z$ are calculated by a nearest neighbour look-up using the shared embedding space $e$ using the following:\n",
    "\n",
    "\\begin{equation}\n",
    "q(z = k|x) = \\begin{cases}\n",
    "1 & \\text{if} \\ k = \\text{argmin}_j || z_e(x) - e_i ||_2 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where $z_e$ is the output of the encoder. This is one-hot encoding of the closest embedding vector and is known as the straight-through estimator. In full, the difference between the continuous and discrete latent representation for one element is:\n",
    "\n",
    "\\begin{equation}\n",
    "(z_i - z_{qi})^2 = z_i^2 + z_{qi}^2 - 2z_i z_{qi}\n",
    "\\end{equation}\n",
    "\n",
    "Across the entire batch,\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} z_i^2 + z_{qi}^2 - 2z_i z_{qi}\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    " = \\frac{1}{N} \\sum_{i=1}^{N} z_i^2 + \\sum_{i=1}^{N} z_{qi}^2 - 2 \\sum_{i=1}^{N} z_i z_{qi}\n",
    "\\end{equation} \n",
    "\n",
    "Using vector notation,\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{N} (||\\textbf{z}||^2 + ||\\textbf{z}_q||^2 - 2\\textbf{z} \\cdot \\textbf{z}_q)\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\n",
    "The input to the decoder in this one-hot encoded latent vector, $e_k$ so that:\n",
    "\n",
    "\\begin{equation}\n",
    "z_q(x) = e_k\n",
    "\\end{equation}\n",
    "\n",
    "where $k = \\text{argmin}_j ||z_e(x) - e_j||_2$.\n",
    "\n",
    "It is clear from the above equation that there is no real gradient defined for $z_q$ which means the gradient cannot flow from the encoder to the decoder through the embedding space during training. How van den Oord et al. fixed this was by using the straight-through estimator: simply, copy the gradients from the decoder input $z_q$ to the encoder output $z_e$. \n",
    "\n",
    "During the forward pass, the nearest embedding $z_q(x)$ is passed to the decoder, and during the backward pass, the gradient of the loss, $\\nabla_z L$ is passed to the encoder to allow updates to the rest of the network to lower the reconstruction loss. This can be done as the output representation of the encoder is the same dimensionality as the input to the decoder (they are sharing the same $D$ dimensional space). \n",
    "\n",
    "The right panel on the figure below from van den Oord et al., shows how the gradient can push the encoder's output to be discretized differently in the next forward pass as the assignment of the embedding vector will be different.\n",
    "\n",
    "The authors of the original VQ-VAE paper introduced more terms to the overall loss function which is written as:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\log p(x|z_q(x)) + ||sg[z_e(x)] - e||_2^2 + \\beta ||z_e(x) - sg[e]||_2^2\n",
    "\\end{equation}\n",
    "\n",
    "The first term is the reconstruction loss which optimizes the encoder and decoder through the straight-through estimator. \n",
    "\n",
    "The second term is a dictionary learning loss term. It penalises the difference between the encoder output and the embedding vectors $e_i$ to move the embedding vectors towards the encoder output. Here sg stands for the 'stop gradient' operator which blocks the flow of gradients and effectively keeps the encoder outputs fixed (only updated by the reconstruction loss, not the partial derivative of this loss term). \n",
    "\n",
    "The volume of the embedding space is dimensionless and therefore, it can grow arbitarily if the embeddings do not train as fast as the encoder parameters. The third term, the commitment loss,  which constrains the growth of the embedding space ensures the encoder commits to an embedding. $\\beta$ here is a hyperparameter for the commitment loss (robust to $0.1 \\leq \\beta \\leq 2.0$) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Vector Quantization](./figures/VQ-VAE_architecture.png)\n",
    "*From van den Oord et al. (2018)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaiton of VQEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class VQEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.num_embeddings = num_embeddings \n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.num_embeddings, 1.0 / self.num_embeddings)\n",
    "    \n",
    "    def forward(self, z, commitment_cost):\n",
    "        b, c, h, w = z.shape # Get the shape (batch_size, embedding_dimension, height, width)\n",
    "\n",
    "        z_channel_last = z.permute(0, 2, 3, 1) # Permute the input to (batch_size, height, width, embedding_dimension)\n",
    "        z_flattened = z_channel_last.reshape(b*h*w, self.embedding_dim) # Flatten the input to (batch_size * height * width, embedding_dimension)\n",
    "        \n",
    "        # Compute the distances between the input and the embeddings\n",
    "\n",
    "        distances = (\n",
    "            torch.sum(z_flattened**2, dim=-1, keepdim=True)\n",
    "            + torch.sum(self.embedding.weight.t()**2, dim=0, keepdim=True)\n",
    "            - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "        )\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)\n",
    "\n",
    "        # Get the quantized latent vectors \n",
    "        z_q = self.embedding(encoding_indices) \n",
    "        z_q = z_q.reshape(b, h, w, self.embedding_dim)\n",
    "        z_q = z_q.permute(0, 3, 1, 2)\n",
    "        \n",
    "        loss = F.mse_loss(z_q, z.detach()) + commitment_cost * F.mse_loss(z_q.detach(), z)\n",
    "\n",
    "        # Straight-through estimator\n",
    "        z_q = z + (z_q - z).detach() \n",
    "\n",
    "        return z_q, loss, encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_embeddings = 3\n",
    "h, w = 3, 3\n",
    "\n",
    "z_e = torch.randn(batch_size, num_embeddings, h, w) # typically encoded inputs from an image (batch_size, embedding_dimension, height, width)\n",
    "commitment_cost = 0.25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqe = VQEmbedding(num_embeddings=num_embeddings, embedding_dim=3)\n",
    "\n",
    "z_q, loss, encoding_indices = vqe(\n",
    "    z_e,\n",
    "    commitment_cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45]) torch.Size([5, 3, 3, 3]) tensor(1.1047, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(encoding_indices.shape, z_q.shape, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Display the first image in the batch\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.title(f'Label: {labels[0].item()}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a series of autoencoders from this chapter leading up to the VAE and VQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.ModuleDict({\n",
    "            'conv1': nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            'conv2': nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            'conv3': nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            'flatten1': nn.Flatten(),\n",
    "            'fc': nn.Linear(64 * 28 * 28, 10)\n",
    "        })\n",
    "\n",
    "        self.decoder = nn.ModuleDict({\n",
    "            'fc': nn.Linear(10, 64 * 28 * 28),\n",
    "            'unflatten': nn.Unflatten(1, (64, 28, 28)),\n",
    "            'conv1': nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            'conv2': nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            'conv3': nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, layer in self.encoder.items():\n",
    "            x = layer(x)\n",
    "            \n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                x = torch.relu(x)\n",
    "            \n",
    "        # Decoder\n",
    "        for layer in self.decoder.values():\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.ConvTranspose2d):\n",
    "                x = torch.relu(x)\n",
    "            print(x.shape)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (64x1024 and 50176x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, data)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/AI algorithms/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI algorithms/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[108], line 27\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 27\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mConv2d):\n\u001b[1;32m     30\u001b[0m             x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/Desktop/AI algorithms/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI algorithms/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI algorithms/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (64x1024 and 50176x10)"
     ]
    }
   ],
   "source": [
    "model = Autoencoder().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.to(device))\n",
    "        loss = loss_fn(outputs, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a batch of test images\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
    "    test_images, _ = next(iter(test_loader))\n",
    "    test_images = test_images.to(device)\n",
    "    \n",
    "    # Reconstruct images\n",
    "    reconstructed = model(test_images)\n",
    "    \n",
    "    # Plot original and reconstructed images\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(test_images[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'autoencoder_model.pth')\n",
    "print(\"Model saved!\")\n",
    "\n",
    "\n",
    "\n",
    "# Need to fix this tomorrow...need to understand the outputs from conv layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
